{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976b15e9-6c4e-43b5-88cf-ecd5a8be02cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Bayesian Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a49bcb-082d-46e3-b920-591541bad759",
   "metadata": {},
   "source": [
    "## Revisiting Bayes' Theorem\n",
    "\n",
    "Basic conditional probability: \n",
    "\n",
    "$p(B|A)=\\frac{p(B\\cap A)}{p(A)}$ (1)\n",
    "\n",
    "Since $p(B|A) \\ne p(A|B)$, BUT $p(B \\cap A) = p(A \\cap B)$: \n",
    "\n",
    "$p(A|B)=\\frac{p(B\\cap A)}{p(B)}$ (2)\n",
    "\n",
    "Then, we derive Bayes' rule: \n",
    "\n",
    "$p(A|B)=\\frac{p(B|A)p(A)}{p(B)}$ (3)\n",
    "\n",
    "Now, let's assume that our model parameters are $\\theta$ and dataset is $y$:\n",
    "\n",
    "$p({\\boldsymbol{\\theta }}|{\\boldsymbol{y}})=\\frac{p({\\boldsymbol{y}}|{\\boldsymbol{\\theta }})p({\\boldsymbol{\\theta }})}{p({\\boldsymbol{y}})}$ (4)\n",
    "\n",
    "Often, we just use the proportional distribution, rather than the fully-normalized posterior:\n",
    "\n",
    "$p({\\boldsymbol{\\theta }}|{\\boldsymbol{y}})\\propto p({\\boldsymbol{y}}|{\\boldsymbol{\\theta }})p({\\boldsymbol{\\theta }})$ (5)\n",
    "\n",
    "Now, we can break this down into different terms:\n",
    "\n",
    "- $p(\\theta)$ is known as the **prior**, probability of the model (hypothesis) before seeing data\n",
    "- $p(\\theta | y)$ is known as the **posterior**, probability of the model (hypothesis) after seeing data\n",
    "- $p(y | \\theta)$ is known as the **likelihood**, probability of the data given a hypothesis\n",
    "\n",
    "**_So, what does this mean in terms of solving a statistical problem?_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a700844-17d0-40ff-aedf-1f2378d5e459",
   "metadata": {},
   "source": [
    "- **Prior:** use any background information we have, or any assumptions we have\n",
    "- **Likelihood:** use any data we can collect to compute probability for each hypothesis\n",
    "- **Posterior:** A result of a **Bayesian update** (using prior probabilities and new data to compute current probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c49ee2-db04-4615-9ab6-e2ddfdb20eb0",
   "metadata": {},
   "source": [
    "## Bayesian statistics treat models as non-permanent (ever evolving) distributions\n",
    "\n",
    "Benefits:\n",
    "\n",
    "1. Prior information is captured\n",
    "2. Updates can increase accuracy of model\n",
    "3. Models are interpretable\n",
    "4. Non-long-run statistics can be captured\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "1. Prior assumptions are weighed in\n",
    "2. More complex setup\n",
    "\n",
    "![Bayesian statistics workflow](bayesian_cycle.png)\n",
    "\n",
    "*Figure 1: The Bayesian statistics workflow works similar to humanistic approaches to research problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2605b4-82b9-4d7d-ad6f-7ef7725586b7",
   "metadata": {},
   "source": [
    "## Updates via Bayes Tables\n",
    "\n",
    "> Suppose there are two bowls of cookies.\n",
    ">\n",
    "> * Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. \n",
    ">\n",
    "> * Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.\n",
    ">\n",
    "> Now suppose you choose one of the bowls at random and, without looking, choose a cookie at random. If the cookie is vanilla, what is the probability that it came from Bowl 1?\n",
    "\n",
    "Let's set this up!  Here are the steps:\n",
    "\n",
    "1. What are our variables and how can we relate them?  In other words, what information do we have, and what are we trying to figure out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf9434-04fa-4fec-baaf-70d1b46cf3f2",
   "metadata": {},
   "source": [
    "- $V$ is an event for receiving a vanilla cookie\n",
    "- $C$ is an event for receiving a chocolate cookie\n",
    "- $B_1$ is an event for receiving a cookie out of Bowl 1\n",
    "- $B_2$ is an event for receiving a cookie out of Bowl 2\n",
    "\n",
    "**Question:** What is $P(B_1|V)$ (the posterior)?  Using our prior, likelihood, and posterior workflow: $P(B_1) P(V|B_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72101676-3ad0-4f2f-90e6-1345a03146ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f2954-791a-4ae6-8b0e-2b1d9d8ce52e",
   "metadata": {},
   "source": [
    "2. Tabularize our information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e42603-27c9-492d-8986-5e1c6656f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cookies = pd.DataFrame(index=['B1', 'B2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263f778-a4bc-47b7-b7d5-420d1051127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "df_cookies['prior'] = Fraction(1, 2), Fraction(1, 2)  # assumption for bowls\n",
    "df_cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41584e0a-48e8-4a19-b638-239fb613fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cookies['likelihood'] = Fraction(30, 40), Fraction(20, 40)\n",
    "df_cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafd1cc-95cb-48b0-80ed-80c9bc9bf82f",
   "metadata": {},
   "source": [
    "3. Multiply to obtain the un-normalized posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc501d3-1785-4e34-9c16-2c347b01700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cookies['unnorm_post'] = df_cookies['prior'] * df_cookies['likelihood']\n",
    "df_cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68418702-8d01-4653-9e85-e8f065e7912b",
   "metadata": {},
   "source": [
    "4. Normalize the posterior by dividing by the total probability so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70d92f-3483-4e52-9275-324a9bb68782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cookies['posterior'] = df_cookies['unnorm_post'] / df_cookies['unnorm_post'].sum()\n",
    "df_cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3339396-31e3-42f5-baa5-6d4d5f28de2e",
   "metadata": {},
   "source": [
    "## The power of Bayesian statistics!\n",
    "\n",
    "### The Monty Hall Problem\n",
    "\n",
    "The Monty Hall problem (one of the most contentious problems in probability!) is based on a game show called *Let's Make a Deal*. If you are a contestant on the show, here's how the game works:\n",
    "1. The host, Monty Hall, shows you three closed doors -- numbered 1, 2, and 3 -- and tells you that there is a prize behind each door.\n",
    "2. One prize is valuable (traditionally a car), the other two are less valuable (traditionally goats).\n",
    "3. The object of the game is to guess which door has the car. If you guess right, you get to keep the car.\n",
    "\n",
    "![Monty hall problem illustrated](monty_hall.png)\n",
    "\n",
    "_Figure 2: The Monty Hall problem illustrated._\n",
    "\n",
    "**The question:** Suppose you pick Door 1. Before opening the door you chose, Monty opens Door 3 and reveals a goat. Then Monty offers you the option to stick with your original choice or switch to the remaining unopened door. _To maximize your chance of winning the car, should you stick with Door 1 or switch to Door 2?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a1afd-4a01-4e7c-b707-95d0c450b642",
   "metadata": {},
   "source": [
    "### Bayes to the rescue! The solution to the Monty Hall problem:\n",
    "\n",
    "To answer this question, we have to make some assumptions about the behavior of the host:\n",
    "\n",
    "1.  Monty always opens a door and offers you the option to switch.\n",
    "\n",
    "2.  He never opens the door you picked or the door with the car.\n",
    "\n",
    "3.  If you choose the door with the car, he chooses one of the other\n",
    "    doors at random.\n",
    "\n",
    "We start with three hypotheses: the car might be behind Door 1, 2, or 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca34759-4eba-4769-b220-9cc6fb3db4fe",
   "metadata": {},
   "source": [
    "According to the statement of the problem, the prior probability for each door is 1/3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c4597-b3a4-4072-a8f4-e86c81fa4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monty = pd.DataFrame(index=['D1', 'D2', 'D3'])\n",
    "df_monty['priors'] = Fraction(1, 3)\n",
    "df_monty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405018a-c0f2-43d0-9419-8c59e31616dc",
   "metadata": {},
   "source": [
    "We also have new data, where Monty opened Door 3 and revealed a goat. Let's convert this data into the probability for each hypothesis (likelihood):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e38d3-de7f-4a01-98b9-f1983d2aee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monty['likelihoods'] = Fraction(1, 2), 1, 0\n",
    "df_monty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b279b-d846-4356-ae2e-ed9ee868dae9",
   "metadata": {},
   "source": [
    "Now, we can perform a Bayesian update (compute the unnormalized posterior and normalized posterior):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f177fd9-61e7-49e2-8aff-1890d63c1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monty['unnorm_posts'] = df_monty['priors'] * df_monty['likelihoods']\n",
    "df_monty['posteriors'] = df_monty['unnorm_posts'] / df_monty['unnorm_posts'].sum()\n",
    "df_monty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c1f54-a5b2-4e87-a86f-66a4bc04fb4c",
   "metadata": {},
   "source": [
    "**Our conclusion:** So, should we stick with Door 1 or switch to Door 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d917c-fcbd-46ae-b46b-3c898edb9176",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distribution functions\n",
    "\n",
    "**pmf: Probability mass function (discrete), $f_X(x) = p(X = x)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef5225-33d8-4e30-a617-9e1c205e363a",
   "metadata": {},
   "source": [
    "**pdf: Probability density function (continuous), $f_X(x) = \\frac{dF_X(x)}{dx}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0ac58-1702-4d76-8480-55d854421c59",
   "metadata": {},
   "source": [
    "**cdf: Cumulative distribution function, $F_X(x) = p(X \\le x)$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5ddcb-c5b7-4c7e-a91c-3951608d3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "# modeling bananas in a bunch: 2 to 8 bananas, uniformly possible\n",
    "example_dist = scipy.stats.randint(2, 9)\n",
    "\n",
    "x = np.arange(0, 10)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.stem(x, example_dist.pmf(x), label='pmf')\n",
    "plt.plot(x, example_dist.cdf(x), label='cdf')\n",
    "plt.grid(linestyle='--')\n",
    "plt.legend(fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f89d0-60b1-4194-b204-a87c101d9f8b",
   "metadata": {},
   "source": [
    "## Bayesian estimation example\n",
    "\n",
    "I often see [rabbits](https://en.wikipedia.org/wiki/Eastern_cottontail) in the garden behind my house, but it's not easy to tell them apart, so I don't really know how many there are.\n",
    "\n",
    "Suppose I deploy a motion-sensing [camera trap](https://en.wikipedia.org/wiki/Camera_trap) that takes a picture of the first rabbit it sees each day.  After three days, I compare the pictures and conclude that two of them are the same rabbit and the other is different.\n",
    "\n",
    "How many rabbits visit my garden? --> What is the chance of 4 rabbits visiting my garden? 5 rabbits? 6 rabbits?\n",
    "\n",
    "To answer this question, we have to think about the prior distribution and the likelihood of the data:\n",
    "\n",
    "* I have sometimes seen four rabbits at the same time, so I know there are at least that many.  I would be surprised if there were more than 10.  So, at least as a starting place, I think a uniform prior from 4 to 10 is reasonable.\n",
    "\n",
    "* To keep things simple, let's assume that all rabbits who visit my garden are equally likely to be caught by the camera trap in a given day.  Let's also assume it is guaranteed that the camera trap gets a picture every day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8259f2-e22e-4677-84b1-55a22b948993",
   "metadata": {},
   "source": [
    "1. Setup the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff888f8-b136-4ae9-8291-4c69cd19038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = np.arange(1, 11)\n",
    "\n",
    "prior_dist = scipy.stats.randint(4, 11)\n",
    "priors = prior_dist.pmf(hypotheses)\n",
    "\n",
    "print(f\"hypotheses: {hypotheses}\")\n",
    "print(f\"priors: {priors}\")\n",
    "\n",
    "plt.stem(hypotheses, priors)\n",
    "plt.title(\"Prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c075e5-bb6e-45e9-ac8d-9f933b58617a",
   "metadata": {},
   "source": [
    "2. Incorporate data into a likelihood distribution.  Our data is that two rabbits were the same, and the third is different. This means that the probability for capturing the same rabbit can be found by multiplying two probabilities:\n",
    "    1. That the second image is the same rabbit as the first: $1/N$ (uniform probability), where $N$ is the number of rabbits.\n",
    "    2. That the third image is a different rabbit as the first two images: $\\frac{N-1}{N}$ (uniform probability complement).\n",
    "\n",
    "Normally, our data is from experiments, but in this case we have a model that is \"simulated\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7b3a5-124e-4934-876d-c1a325cacde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = hypotheses\n",
    "day2_likelihoods = (1 / hypotheses)  # day 2 update\n",
    "# print(f\"day 2 likelihoods: {day2_likelihoods}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea334a-7871-4450-be9b-6d7ce0c47313",
   "metadata": {},
   "source": [
    "3. Now, perform our Bayesian update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ef05b-ca48-47c6-a22c-c1232adb72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "day2_unnorm_posts = priors * day2_likelihoods\n",
    "# print(f\"day 2 unnormalized posteriors: {day2_unnorm_posts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba41ea-a62d-470f-a1d9-907c3435439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "day2_posteriors = day2_unnorm_posts / day2_unnorm_posts.sum()\n",
    "print(f\"day 2 posteriors: {day2_posteriors}\")\n",
    "plt.scatter(hypotheses, priors, label=\"Original priors\")\n",
    "plt.scatter(hypotheses, day2_posteriors, label=\"Day 2 posteriors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a946d-337c-42d6-a2c8-81ae5388ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "day3_priors = day2_posteriors.copy()\n",
    "day3_likelihoods = (hypotheses - 1) / hypotheses\n",
    "day3_unnorm_posts = day3_priors * day3_likelihoods\n",
    "# print(f\"day 3 unnormalized posteriors: {day3_unnorm_posts}\")\n",
    "\n",
    "day3_posteriors = day3_unnorm_posts / day3_unnorm_posts.sum()\n",
    "print(f\"day 3 posteriors: {day3_posteriors}\")\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.scatter(hypotheses, priors, label=\"Original priors\")\n",
    "plt.scatter(hypotheses, day2_posteriors, label=\"Day 2 posteriors\")\n",
    "plt.scatter(hypotheses, day3_posteriors, label=\"Day 3 posteriors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafeb159-0630-4158-b5a7-5316d045fe6a",
   "metadata": {},
   "source": [
    "Create a specialized class (data type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b747e-758d-4876-a2f9-f658a7f692eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianEstimation(object):\n",
    "    def __init__(self, hypotheses: np.ndarray, prior_dist: scipy.stats.rv_discrete):\n",
    "        \"\"\"\n",
    "        BayesianEstimation(hypotheses, prior_dist) builds a Bayesian estimation\n",
    "          data structure that helps with computing Bayesian estimation problems.\n",
    "\n",
    "        :param hypotheses: numpy array representing the hypotheses\n",
    "        :param prior_dist: distribution for the priors\n",
    "        \"\"\"\n",
    "        self.hypotheses = hypotheses\n",
    "        self.prior_dist = prior_dist\n",
    "        self.priors = self.prior_dist.pmf(self.hypotheses)\n",
    "        self.posteriors = [self.priors]\n",
    "\n",
    "    def update(self, likelihood: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        update(likelihood) updates the posteriors and returns the normalized posterior\n",
    "        \"\"\"\n",
    "        self.posteriors.append(self.posteriors[-1] * likelihood)\n",
    "        self.posteriors[-1] = self.posteriors[-1] / self.posteriors[-1].sum()\n",
    "        return self.posteriors[-1]\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        plot() plots all the posteriors\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        plt.scatter(hypotheses, priors, label=\"Priors\")\n",
    "        # for idx in range(len(self.posteriors)):\n",
    "        #     self.posteriors[idx]\n",
    "        for idx, item in enumerate(self.posteriors):\n",
    "            plt.plot(hypotheses, item, label=f\"Posterior {idx}\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6265fe-2a28-4907-90ff-403c26395206",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = np.arange(1, 11)\n",
    "prior_dist = scipy.stats.randint(4, 11)\n",
    "\n",
    "my_rabbit_problem = BayesianEstimation(hypotheses, prior_dist)\n",
    "my_rabbit_problem.update(1 / my_rabbit_problem.hypotheses)\n",
    "my_rabbit_problem.update((my_rabbit_problem.hypotheses - 1) / my_rabbit_problem.hypotheses)\n",
    "my_rabbit_problem.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77e86a-7a66-44a3-bca0-632f0d3f7af5",
   "metadata": {},
   "source": [
    "Just like with frequentist estimation, we want to know how confident we can be! Enter the Bayesian version of a confidence interval, the 90% **credible interval**, which are the quantities that bound the middle 90% probability, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11255485-80cc-48ec-9934-9cdd7dce3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile(df_posterior: pd.DataFrame, col_probs: str,\n",
    "                 col_hypos: str, desired_prob: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the quantile for the desired probability.\n",
    "    \"\"\"\n",
    "    acc_probs = 0.0\n",
    "    for _, row in df_posterior.iterrows():\n",
    "        acc_probs += row[col_probs]\n",
    "        if acc_probs >= desired_prob:\n",
    "            return row[col_hypos]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe87923-7023-43fc-b056-045232092ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rabbits = {\n",
    "    \"number of rabbits\": my_rabbit_problem.hypotheses,\n",
    "    \"probabilities\": my_rabbit_problem.posteriors[-1]\n",
    "}\n",
    "\n",
    "df_rabbits = pd.DataFrame.from_dict(dict_rabbits)\n",
    "df_rabbits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4f0b5-65ab-497f-8aa2-83e00d34fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantile(df_rabbits, \"probabilities\", \"number of rabbits\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a773d71-7317-4beb-8d6f-e3c5153c025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantile(df_rabbits, \"probabilities\", \"number of rabbits\", 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39bb95-2736-4cab-84d6-6cc3b1e19bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rabbits.loc[df_rabbits['probabilities'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03036c40-14db-4cf3-94c3-cb9c47f005ff",
   "metadata": {},
   "source": [
    "**What can we draw from this?**\n",
    "\n",
    "1. Our credible interval is incredibly large.  We probably need more data.\n",
    "2. If we adjust the prior, we will get vastly different posteriors. The prior we have is called an uninformative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a448f37-cdc7-4154-a5f9-90bf3d9d1e54",
   "metadata": {},
   "source": [
    "## Bayesian regression\n",
    "\n",
    "### Installing PyMC3 properly\n",
    "\n",
    "It turns out that the `conda-forge` version of `PyMC3` is [slightly broken](https://github.com/pymc-devs/pymc/wiki/Installation-Guide-(Windows)) on Windows.  \n",
    "\n",
    "For Windows users with normal processors (Intel Core series or AMD Ryzen series), we have to do the following:\n",
    "\n",
    "1. Make sure everything is updated: `mamba upgrade --all`\n",
    "2. Use `mamba` to install: `libpython mkl-service m2w64-toolchain numba python-graphviz arviz dill cachetools patsy semver typing-extensions cachetools fastprogress xarray filelock netcdf4 libblas blas lapack`\n",
    "3. Then, use `pip` to install `pymc3`.  This is necessary to trigger compilation of PyMC3, which isn't being triggered by the conda-forge version for some reason.\n",
    "4. Finally, we'll also install `bambi` using `pip`. `bambi` is an easier way to use `PyMC3` to create Bayesian models once we are comfortable with the pymc3 underpinnings.\n",
    "\n",
    "For Windows users with workstation/server processors (Intel Xeon series or AMD Epyc series), we have to do the following:\n",
    "\n",
    "1. Make sure everything is updated: `mamba upgrade --all`\n",
    "2. Use `mamba` to install: `libpython mkl-service numba python-graphviz arviz dill cachetools patsy semver typing-extensions cachetools fastprogress xarray filelock netcdf4 libblas blas lapack` (notice that we will NOT install m2w64-toolchain).\n",
    "3. Install MSYS2 following its directions: https://www.msys2.org/\n",
    "4. Add your path for MSYS2's MinGW64 bin folder (for instance, mine is `C:\\msys64\\mingw64\\bin`) to the user PATH.  Then, close and re-open mambaforge prompt.\n",
    "5. Then, use `pip` to install `pymc3`.  This is necessary to trigger compilation of PyMC3, which isn't being triggered by the conda-forge version for some reason.\n",
    "6. Finally, we'll also install `bambi` using `pip`. `bambi` is an easier way to use `PyMC3` to create Bayesian models once we are comfortable with the pymc3 underpinnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d394b7-693d-4e0d-9030-188b9647ff4f",
   "metadata": {},
   "source": [
    "### Example Generalized Linear Model problem with Ordinary Linear Regression\n",
    "\n",
    "**Dataset:** `https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv`\n",
    "\n",
    "**Dataset info:** The five core elements are:\n",
    "- PRCP = Precipitation (tenths of mm)\n",
    "- SNOW = Snowfall (mm)\n",
    "- SNWD = Snow depth (mm)\n",
    "- TMAX = Maximum temperature (tenths of degrees C)\n",
    "- TMIN = Minimum temperature (tenths of degrees C)\n",
    "\n",
    "\n",
    "**Research question:** Can we create a Bayesian regression model to predict snowfall?\n",
    "\n",
    "Note that regression is still: $y = ax + b + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c34c1-61c0-4af1-a2e2-9c311acbe304",
   "metadata": {},
   "source": [
    "First, we create the DataFrame as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ee959-d2fb-4bfd-b934-af00451909c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_snowfall = pd.read_csv(\"https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv\")\n",
    "columns_to_keep = ['DATE', 'PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN']\n",
    "# df_snowfall = df_snowfall.drop(columns=[col for df_snowfall.columns if col not in columns_to_keep], axis=1)\n",
    "df_snowfall = df_snowfall[columns_to_keep].copy()  # copy to make it a brand new dataframe\n",
    "print(len(df_snowfall))\n",
    "df_snowfall.tail()\n",
    "\n",
    "def get_year(date_string: str) -> int:\n",
    "    return int(date_string[0:4])\n",
    "\n",
    "def get_month(date_string: str) -> int:\n",
    "    return int(date_string[5:7])\n",
    "\n",
    "def get_day(date_string: str) -> int:\n",
    "    return int(date_string[8:])\n",
    "\n",
    "# fix the date column into separate YEAR, MONTH, DAY columns\n",
    "df_snowfall['YEAR'] = df_snowfall['DATE'].apply(get_year)\n",
    "df_snowfall['MONTH'] = df_snowfall['DATE'].apply(get_month)\n",
    "df_snowfall['DAY'] = df_snowfall['DATE'].apply(get_day)\n",
    "df_snowfall = df_snowfall.drop(columns=['DATE'], axis=1)\n",
    "\n",
    "df_snowfall = df_snowfall.dropna()\n",
    "\n",
    "df_snowfall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b1193-1fe0-4e6b-8ca1-f864790116aa",
   "metadata": {},
   "source": [
    "Next, explore the dataset as usual, using plots (jointplot, etc.) as needed.  We can use `arviz`, which is a plotting library similar to `seaborn`, but meant primarily for Bayesian models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828295c-13c2-4122-bf74-41865aa7e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df_snowfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82df259-77f2-4d2b-a264-30d74f02be8d",
   "metadata": {},
   "source": [
    "Now, we can use `pymc3` (similar to `statsmodels` but for Bayesian statistics) to build a specific Bayesian regression model (Generalized Linear Model) for us.  We build our model using the same steps as with all of Bayesian statistics:\n",
    "1. Choose a distribution for the prior.\n",
    "2. Pick how we want to model the likelihood of our parameter (either $a$, $b$, or $\\epsilon$).\n",
    "3. Run a sampler, such as Markov Chain Monte Carlo, to sample the posterior. This sampling is required because computing every single possible density can take significant resources.  Let's discuss why!\n",
    "4. Analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154735b8-9b36-47e3-bd65-961ed5260693",
   "metadata": {},
   "source": [
    "#### Using the PyMC3 formula interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0973b5-728d-4d1d-8e81-99cfd9f5e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "print(theano.config.cxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ed69d-7bae-4ae3-b314-aaf2c5717516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz\n",
    "\n",
    "df_trace = None  # memory placeholder\n",
    "trace = None # memory placeholder\n",
    "\n",
    "with pymc3.Model() as snowfall_model:  # context manager in Python\n",
    "    likelihood_family = pymc3.glm.families.Normal()\n",
    "    pymc3.GLM.from_formula(formula=\"SNOW ~ 1 + MONTH + np.power(MONTH, 2)\", \n",
    "                           data=df_snowfall, family=likelihood_family)\n",
    "    trace = pymc3.sample(init=\"adapt_diag\", random_seed=42, \n",
    "                         return_inferencedata=True)\n",
    "    print(\"this still has access to the model\")\n",
    "    df_trace = arviz.summary(trace)\n",
    "    arviz.plot_trace(trace, figsize=(10, 10), legend=True)\n",
    "\n",
    "print(\"hello this no longer has access to the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd1da1-ca3f-4b13-b53b-e90d6052a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24ec63-4c04-46c1-addb-149c5f6e295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(df_snowfall[\"MONTH\"], df_snowfall[\"SNOW\"], label=\"data\", alpha=0.02)\n",
    "\n",
    "y_model = df_trace[\"mean\"][\"np.power(MONTH, 2)\"] * np.power(df_snowfall[\"MONTH\"], 2) \n",
    "y_model += df_trace[\"mean\"][\"MONTH\"] * df_snowfall[\"MONTH\"] \n",
    "y_model += df_trace[\"mean\"][\"Intercept\"]\n",
    "plt.scatter(df_snowfall[\"MONTH\"], y_model, label=\"prediction\")\n",
    "\n",
    "plt.title(\"Posterior predictive regression lines\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c692396-1a61-42c4-ac39-ef7be46a1521",
   "metadata": {},
   "source": [
    "#### Using the `bambi` formula interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321601fc-8f02-4fe5-b4f1-b3473efb46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bambi as bmb\n",
    "\n",
    "lin_model = bmb.Model(\"SNOW ~ np.power(MONTH, 2) + MONTH + 1\", data=df_snowfall)\n",
    "lin_model_fit = lin_model.fit(tune=2000, draws=2000, init=\"adapt_diag\", \n",
    "                              pickle_backend=\"dill\", cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9766e-24ba-452c-a806-819ee0caf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.plot_priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f31e3f-d468-456d-b2e8-b03b0791ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e37e53-a6e4-44e7-91c3-a3e43d4c5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_trace(lin_model_fit, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75fcf8-a881-40e1-ad10-a9e12d6872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lin_model_fit = arviz.summary(lin_model_fit)\n",
    "df_lin_model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfd8b20-de97-4ebe-ac40-0b9303ed40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(df_snowfall[\"MONTH\"], df_snowfall[\"SNOW\"], \"x\", label=\"data\", alpha=0.1)\n",
    "x_range = np.linspace(df_snowfall[\"MONTH\"].min(), df_snowfall[\"MONTH\"].max(), 2000)\n",
    "y_pred = df_lin_model_fit[\"mean\"][\"Intercept\"]\n",
    "y_pred += df_lin_model_fit[\"mean\"][\"np.power(MONTH, 2)\"] * (np.power(x_range, 2))\n",
    "y_pred += df_lin_model_fit[\"mean\"][\"MONTH\"] * x_range\n",
    "plt.plot(x_range, y_pred, label=\"Recovered regression line\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655ef8f-80cc-4e3a-9ec6-e229224d8dd5",
   "metadata": {},
   "source": [
    "#### Using the PyMC3 object-oriented interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d8144-9075-40de-8acb-fc9e0bfe7366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b85e7c-60e9-4543-ba11-1409abf6f443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3d852b3-7a32-4eee-b066-2701d020a790",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example GLM problem using Robust Linear Regression\n",
    "\n",
    "**Dataset:** `https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv`\n",
    "\n",
    "**Research question:** Can we create a Bayesian regression model to predict snowfall?\n",
    "\n",
    "**Why do we want to use Robust Linear Regression?** Robust Linear Regression uses the t-distribution instead of the Normal distribution, meaning that outliers do not affect the model as strongly.\n",
    "\n",
    "![Robust vs. Ordinary Linear Regression](robust_vs_normal.png)\n",
    "\n",
    "Note that regression is still: $y = ax + b + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e94827-c166-4eb3-9bc8-c1ffd42cc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bambi as bmb\n",
    "\n",
    "rob_model = bmb.Model(\"SNOW ~ np.power(MONTH, 2) + MONTH + 1\", data=df_snowfall,\n",
    "                      family=\"t\")\n",
    "rob_model_fit = rob_model.fit(tune=2000, draws=2000, init=\"advi\",\n",
    "                              pickle_backend=\"dill\", cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795509b0-595a-4f2d-ae61-f657039a475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3671ba6-7153-4a58-b196-50effcd77c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f122bf-36bb-4ec0-b375-a3be65edf7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ad3505-4ab4-4c74-889d-5c5930634b9c",
   "metadata": {},
   "source": [
    "### Example GLM problem using Hierarchical Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da01108-4d18-41a5-92c8-0b9c4c06fd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929fdf5-721f-4085-97a9-7a8f872ff7f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb8b4b-017f-45ca-8bbb-8835b050d825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9e104-57ea-47be-b52d-e35dce45c982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
<<<<<<< Updated upstream
   "version": "3.8.12"
=======
   "version": "3.9.7"
>>>>>>> 81e852205971a37d71d85845fecc0cf95dbac6df
  },
  "toc-autonumbering": true
=======
   "version": "3.9.6"
  }
>>>>>>> Stashed changes
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
